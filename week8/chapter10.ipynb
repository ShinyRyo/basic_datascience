{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"chapter10.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"P3761Pm_11eA","colab_type":"text"},"source":["# 10 データサイエンティスト中級者への道"]},{"cell_type":"markdown","metadata":{"id":"jD_XYWQ111eD","colab_type":"text"},"source":["\n","この章では、データサイエンティスト入門レベルを卒業し、中級者の道に進むためのさまざまな分析の手法やアプローチ、ツールについて紹介します。\n","\n","一部実装もありますが、最低限の基礎的な説明や概念の紹介が中心となります。深層学習を学ぶために必要となる基礎知識、Pythonの処理を高速化するためのツール群、膨大なデータをサーバーに分散処理させるためのSparkの紹介など、今後データ分析をする上で身につけておきたいスキルばかりです。\n","\n","はじめは理解しにくい箇所もあるかもしれませんが、このようなアプローチやツールがあるということを知っておくだけでも、今後の学習にきっと役に立つはずです。\n","\n","なお、本章は基本的に紹介のみですので、用語の解説やサンプルコードは少ししかありません。ここで紹介した手法やツール等については、あくまでリファレンス程度のものです。これらのことを本格的に学びたい場合は、本章だけのコンテンツでは不十分です。参考文献等を読んだり、他の講座で勉強して、どんどんスキルを磨いていってください。\n","\n","\n","ゴール：データサイエンティスト中級者になるためのさまざまなアプローチやツールを知る（深層学習の基礎知識の習得、Pythonの処理で並立処理やCython、Spark等を活用して高速化するための方法を知る）"]},{"cell_type":"markdown","metadata":{"id":"T8IkiMoU11eG","colab_type":"text"},"source":["- **[10.1 この章の概要](#10.1-この章の概要)**\n","<br><br>\n","- **[10.2 深層学習を学ぶための準備](#10.2-深層学習を学ぶための準備)**\n","    - [10.2.1 パーセプトロン](#10.2.1-パーセプトロン)\n","    - [10.2.2 ニューラルネットワーク](#10.2.2-ニューラルネットワーク)\n","    - [10.2.3 確率的勾配降下法と誤差逆伝播法](#10.2.3-確率的勾配降下法と誤差逆伝播法)\n","    - [10.2.4 深層学習のライブラリ](#10.2.4-深層学習のライブラリ)\n","<br><br>\n","- **[10.3 Pythonの高速化](#10.3-Pythonの高速化)**\n","    - [10.3.1 並列処理](#10.3.1-並列処理)\n","    - [10.3.2 Numba入門](#10.3.2-Numba入門)\n","    - [10.3.3 Cython入門](#10.3.3-Cython入門)\n","<br><br>\n","- **[10.4 Spark入門](#10.4-Spark入門)**\n","    - [10.4.1 PySpark入門](#10.4.1-PySpark入門)\n","    - [10.4.2 SparkSQL](#10.4.2-SparkSQL)\n","<br><br>\n","- **[10.5 その他の数学的手法とエンジニアリングツール](#10.5-その他の数学的手法とエンジニアリングツール)**\n","    - [10.5.1 数学的手法](#10.5.1-数学的手法)\n","    - [10.5.2 エンジニアリングツール](#10.5.2-エンジニアリングツール)\n","<br><br>\n","- **[10.6 総合問題](#10.6-総合問題)**\n","    - [■総合問題10-1 深層学習の用語](#■総合問題10-1-深層学習の用語)\n","    - [■総合問題10-2 Pythonの高速化とSparkに関する用語](#■総合問題10-2-Pythonの高速化とSparkに関する用語)\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"Ucgw4Cw411eH","colab_type":"text"},"source":["## 10.1 この章の概要\n","キーワード：深層学習、ディープラーニング、パーセプトロン、勾配降下法、誤差逆伝播法、前処理、Cython、Spark"]},{"cell_type":"markdown","metadata":{"id":"1qGCKA6d11eI","colab_type":"text"},"source":["この章ではまず、最近注目されている深層学習（ディープラーニング）を学ぶための前準備として、いくつか押さえておきたい基礎概念や実装を紹介します。この講座で学んだ統計知識（最尤法、最小二乗法など）や機械学習の考え方（教師あり学習、教師なし学習、交差検証法、混同行列など）と、ここで学ぶ概念（パーセプトロン、勾配降下法、誤差逆伝播法など）を勉強しておけば、深層学習を学ぶ前の良い準備となるでしょう。"]},{"cell_type":"markdown","metadata":{"id":"atp2mnDd11eJ","colab_type":"text"},"source":["次に、機械学習や深層学習等のモデリングの話から離れて、Pythonの処理を高速化するための方法をいくつか紹介します。\n","\n","モデル作成の前にデータ加工をする必要（いわゆる前処理）が多々ありますが、その前処理の計算時間を改善することも大事です。もちろん、コーディングスキルによって計算処理時間は異なってくるのですが、そのアルゴリズム作成にも限界があります。ここで紹介するライブラリ等を使うことで、計算コストを下げることができます。\n","\n","一昔前、Pythonはスクリプト言語だから遅いという指摘もありました。ただ、昨今、Cythonや並列処理などの色々なライブラリが出てきており、下手なCプログラムを書くよりも、Pythonで実装したほうがいいこともあります。すべての高速化処理について紹介することはできず、ここで紹介する実装が必ず最適というわけではありませんが、計算時間に課題があったときには、ぜひここで使うライブラリ等を使って計算速度をあげることも検討してください。"]},{"cell_type":"markdown","metadata":{"id":"lcFGXy5E11eK","colab_type":"text"},"source":["さらに、分散処理をするためのSparkについて説明します。\n","\n","ご承知の通り、現代の世の中には大量のデータがあり、そのデータは日々蓄積されています。この講座でもある程度、こうしたビッグデータに対応できるようなスキルを身に付けるため、いろいろなツールやアプローチを紹介してきました。ここではさらに、そうした膨大なデータに対して、データの加工から機械学習まで一貫して処理し、複数のサーバーを使った分散処理で計算スピードを上げ、さらにリアルタイムに分析ができるSparkを紹介します。今回は入門ということで、その機能と使い方をみていきます。"]},{"cell_type":"markdown","metadata":{"id":"x4VKNB2411eL","colab_type":"text"},"source":["そして最後に、今後データ分析業務をやっていく上で必要となってくるであろう数学的な手法や、エンジニアリングツール等を紹介します。\n","\n","数学的な手法では、実験計画法やMCMC、エンジニアリング面では、Linuxやクラウドサービスなどを簡単に紹介します。もちろん、ここですべてが網羅されているわけではありませんが、ビジネスの現場で使われている手法やツールですので、ぜひ参考にしてください（ただし、これらの手法やツールにとらわれることなく、課題に対する理解や最適なアプローチは何であるか、常に考えていくことが大事だということは忘れないでください）。"]},{"cell_type":"markdown","metadata":{"id":"JpNhiE8S11eM","colab_type":"text"},"source":["## 10.2 深層学習を学ぶための準備\n","ゴール：深層学習を学ぶための基礎知識をおさえる\n","\n","\n","深層学習は、ニューラルネットワークを用いた学習の応用です。深層学習を学ぶにあたっては、その基本となる論理回路やパーセプトロンなどの基礎を理解しておきましょう。\n"]},{"cell_type":"markdown","metadata":{"id":"6esqXvYu11eN","colab_type":"text"},"source":["### 10.2.1 パーセプトロン\n","キーワード：パーセプトロン、論理回路"]},{"cell_type":"markdown","metadata":{"id":"8IGKDD_j11eO","colab_type":"text"},"source":["まずは、ニューラルネットワークや深層学習の基礎となるパーセプトロンについて学びましょう。パーセプトロンは、複数の値を受け取って処理をし、1つの結果を返す関数です。"]},{"cell_type":"markdown","metadata":{"id":"U6a098yD11eP","colab_type":"text"},"source":["#### AND論理回路の例\n","例として、ANDの論理回路（論理ゲート）の実装を考えます。AND関数は、たとえば0か1が入力されたとき、どちらも1の場合に1を返す関数です。以下の図が参考になります。\n","\n","AND関数の回路記号や真理表（入力された0と1の演算と出力を対応させた表）は一番上の行にあります。真理表の具体的な見方は、たとえば、AとBが両方とも1の場合に、アウトプットとなるYが1になるというのが、表からわかります。"]},{"cell_type":"markdown","metadata":{"id":"9n6xb9F_11eQ","colab_type":"text"},"source":["![comment](https://image.jimcdn.com/app/cms/image/transf/dimension=661x10000:format=png/path/s9a246d2d2c830e8d/image/i77bdab51b44d4889/version/1424716737/image.png)"]},{"cell_type":"markdown","metadata":{"id":"MvDfWi5C11eR","colab_type":"text"},"source":["参照URL:https://image.jimcdn.com/app/cms/image/transf/dimension=661x10000:format=png/path/s9a246d2d2c830e8d/image/i77bdab51b44d4889/version/1424716737/image.png"]},{"cell_type":"markdown","metadata":{"id":"SB0syH-j11eS","colab_type":"text"},"source":["#### パーセプトロンでAND関数を作る\n","\n","以下に示すのはAND関数をパーセプトロンとして構成した例です。ある閾値（`theta`）と入力された値に対する重み（`w1`、`w2`）が設定されており、それらの入力値と重み付けの演算結果（`tmp`）がその閾値を超えるかどうかで結果を判定をしています。"]},{"cell_type":"code","metadata":{"id":"pcxKUobE11eT","colab_type":"code","colab":{}},"source":["def and_func(x1, x2):\n","    w1, w2, theta = 0.5, 0.5, 0.7\n","    tmp = x1 * w1 + x2 * w2\n","    if tmp <= theta:\n","        return 0\n","    elif tmp > theta:\n","        return 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Vd4XwQ611eX","colab_type":"text"},"source":["上記の関数の実行結果は以下となります。たとえば、入力を0と1にしたときの出力は0、入力を1と1にしたときの出力は1となり、結果はAND演算のようになっていることがわかります。"]},{"cell_type":"code","metadata":{"id":"WNQaxbdP11eZ","colab_type":"code","colab":{}},"source":["print('0 かつ 0:', and_func(0, 0))\n","print('0 かつ 1:', and_func(0, 1))\n","print('1 かつ 0:', and_func(1, 0))\n","print('1 かつ 1:', and_func(1, 1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pxPCPVgA11ec","colab_type":"text"},"source":["上記はとてもシンプルな実装で、特定の重みで入力値を0と1に限定すると先ほどのような論理ゲートを模倣できます。そして、複数のパーセプトロンをつなぎ合わせることで、ニューラルネットワーク（後述）を構築できます。\n","\n","また、上の図をみていただくとわかる通り、他には、OR関数やNOT関数などもあります。これらの関数の実装などについて詳しく知りたい方は参考文献「A-29」をご覧ください。"]},{"cell_type":"markdown","metadata":{"id":"QjxcuFQY11ed","colab_type":"text"},"source":[">**[やってみよう]**\n",">\n",">OR関数はどのように実装しますか。考えて、実装してみましょう。\n"]},{"cell_type":"markdown","metadata":{"id":"26J1pPDx11ee","colab_type":"text"},"source":["#### <練習問題 10-1>\n","\n","NAND（NOT AND）関数を作成して、それが合っているかどうか確かめてください。"]},{"cell_type":"markdown","metadata":{"id":"o2UkHpA611ef","colab_type":"text"},"source":["### 10.2.2 ニューラルネットワーク\n","キーワード：ニューラルネットワーク、活性化関数"]},{"cell_type":"markdown","metadata":{"id":"qAc02HpU11eg","colab_type":"text"},"source":["パーセプトロンを学んだら、ニューラルネットワークを学びましょう。\n","\n","ニューラルネットワークは、順伝播型ネットワークやフォワードネットワークともいわれ、層状に並べたユニットが、隣り合った層と結合しており、入力情報から出力情報に一方的に伝播していくモデルです。先ほどのパーセプトロンでは2つの入力値を使ってシンプルな条件式で判定して出力を決定していましたが、ニューラルネットワークでは活性化関数と言われる関数を使って計算して、出力を決定します。"]},{"cell_type":"markdown","metadata":{"id":"Hq7y7fTb11eh","colab_type":"text"},"source":["以下の参照URLを参考にしてください。上側の図では、入力値$X_1$から$X_n$が、$W_1$から$W_n$によって重みづけされ（掛け算され）、$f$を関数として計算をして、その結果を出力します。下側の図は、層が複数になっているイメージです。"]},{"cell_type":"markdown","metadata":{"id":"TgVlwtMX11ei","colab_type":"text"},"source":["![comment](https://thinkit.co.jp/images/article/30/2/3021.gif)"]},{"cell_type":"markdown","metadata":{"id":"EeuIn6FM11ej","colab_type":"text"},"source":["参照URL:https://thinkit.co.jp/images/article/30/2/3021.gif"]},{"cell_type":"markdown","metadata":{"id":"aO2ff1WE11ek","colab_type":"text"},"source":["活性化関数には、シグモイド関数（下図参照）、正規化線形関数等が使われます。シグモイド関数は、出力が0から1の間に緩やかに変化する関数です。詳細は、参考文献「A-29」で紹介している『深層学習』などをみてください。"]},{"cell_type":"markdown","metadata":{"id":"ExF16bre11ek","colab_type":"text"},"source":["![comment](https://image.slidesharecdn.com/deep-learning-20140130-140130205750-phpapp01/95/deep-learning-12-638.jpg?cb=1391115802)"]},{"cell_type":"markdown","metadata":{"id":"vnFdMknp11en","colab_type":"text"},"source":["参照URL:https://image.slidesharecdn.com/deep-learning-20140130-140130205750-phpapp01/95/deep-learning-12-638.jpg?cb=1391115802"]},{"cell_type":"markdown","metadata":{"id":"G2u1itFU11eo","colab_type":"text"},"source":["### 10.2.3 確率的勾配降下法と誤差逆伝播法\n","キーワード：勾配（降下）法、バッチ学習、ミニバッチ学習、確率的勾配降下法、誤差逆伝播法"]},{"cell_type":"markdown","metadata":{"id":"4RtYhpww11ep","colab_type":"text"},"source":["私たちがやりたいのは、誤差関数を最小化する値（パラメータ）を求めることです。そのアプローチ方法をさらに詳しく紹介します。"]},{"cell_type":"markdown","metadata":{"id":"F5VeuN_511eq","colab_type":"text"},"source":["#### 勾配法\n","\n","誤差関数を最小化する値を探すとき、簡単な関数（二次関数の放物線など）だと求めやすいのですが、複雑な関数になってくると、どこで最小値を取るかを解析的に求めることが困難なケースがほとんどです。\n","\n","そのようなときに、まずはどこかに値をとって、値が小さくなる方向の勾配を求めて最小値を探していくのが**勾配法**です。これを繰り返し計算し、最小値がどこにあるのか探していきます。数式の傾きがその方向を決めるため、数学的には関数の偏微分を求めます。なお、最小値を求めるために下方向に向かって値を収束させることが多いので、勾配降下法といったりもします。"]},{"cell_type":"markdown","metadata":{"id":"-Wmoib3F11er","colab_type":"text"},"source":["#### 確率的勾配降下法\n","\n","さてデータを使って学習させるのに、訓練データをすべて使って一気に学習をさせる方法を**バッチ学習**といいます。従来はこのアルゴリズムが採用されていました。しかし、今日のデータ状況からみて、膨大なデータをすべて使って学習させるのには、かなりのコスト（時間やお金）がかかります。\n","\n","そこで、訓練データの中から無作為にデータを選んで、そのデータを使って学習をしていく（重み付けしていく）方法である**ミニバッチ学習**が使われています。このアプローチを取れば、大量なデータをすべて処理する必要がなくなり、計算コストを下げることができます。なお、データ1つ1つを取り出して学習させる**オンライン学習**もありますが、ここでは省略します。"]},{"cell_type":"markdown","metadata":{"id":"JJCwLRTN11es","colab_type":"text"},"source":["**確率的勾配降下法**は、このミニバッチ学習（訓練データのサンプルを1つまたは一部）を使って、勾配降下法でパラメータの更新をしていく法です。すべてのデータを使っていないため、計算効率が向上し、さらに局所的な解になってしまうリスクを抑えることができます。"]},{"cell_type":"markdown","metadata":{"id":"vlayqvbC11et","colab_type":"text"},"source":["##### 誤差逆伝播法\n","\n","ただし、確率的勾配降下法であっても、改善できるとはいえ、計算時間がかかることもあります。それを解決する方法として、**誤差逆伝播法**があります。これは、重み付けのパラメータの勾配計算を効率良く行う手法です。逆伝播とは、出力層側から入力層に誤差情報を伝えていくことをいい、誤差逆伝播法はこのアプローチをとります。上記の図（10.2.2の最初の図）にて、下の後ろ向き演算がそのイメージです。"]},{"cell_type":"markdown","metadata":{"id":"9kOTSDw811ew","colab_type":"text"},"source":["以上で、概念的な紹介は終わりです。さらに学びたい方には、上で紹介した参考文献「A-29」の『ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装』などがオススメです。"]},{"cell_type":"markdown","metadata":{"id":"dn5nr6Lf11ex","colab_type":"text"},"source":["### 10.2.4 深層学習のライブラリ\n","キーワード：Chainer、Theano、Pytorch、Tensorflow"]},{"cell_type":"markdown","metadata":{"id":"5xLCURxD11ey","colab_type":"text"},"source":["さて、これまでは深層学習の前学習ということで、パーセプトロンや確率的勾配降下法などを学びました。今後、深層学習を学ぶ方は、ChainerやTheano、Tensorflow、Pytorchという深層学習の計算を実行するためのライブラリを使っていくことになりますので、ここで少し紹介します。\n"]},{"cell_type":"markdown","metadata":{"id":"M8uABVDx11ey","colab_type":"text"},"source":["- Chainer\n","\n","株式会社Preferred Networksが開発するディープラーニングのフレームワークです。いろいろなニューラルネットワークの構造に対応しており、GPUもサポートしています。詳細は、以下のサイトをご覧ください。\n","\n",">[参考URL]\n",">\n",">http://chainer.org/"]},{"cell_type":"markdown","metadata":{"id":"AgNHxabB11ez","colab_type":"text"},"source":["- Theano\n","\n","モントリオール大学のBengio教授によって開発されている数値計算をするためのライブラリです。ディープラーニング自体の実装ではなく、それを計算するためのいろいろなサポート（微分計算、コンパイラ）などをしています。こちらもGPUで使うことができます（2017年11月の1.0リリースを以て開発が終了しています）。\n","\n",">[参考URL]\n",">\n",">http://deeplearning.net/software/theano/"]},{"cell_type":"markdown","metadata":{"id":"liSzzLHY11e0","colab_type":"text"},"source":["- Pytorch\n","\n","Pytorchは2つの特徴をもったPythonのライブラリで、1つ目がGPUを使ったテンソル計算ができることと、2つ目が深層学習の計算ができるということです。深層学習のライブラリとしては後発ですが、人気がでているようです。なお、テンソルとはベクトルや行列をさらに一般化した概念で、多次元配列のデータを扱います。\n","\n","\n",">[参考URL]\n",">\n",">https://pytorch.org/"]},{"cell_type":"markdown","metadata":{"id":"ilFqPl8_11e1","colab_type":"text"},"source":["- Tensorflow\n","\n","Googleがリリースした機械学習やディープラーニングのライブラリです。さまざまなOSに対応しています。Googleの社内での研究やサービスの開発において実際に使われており、公開後に利用者数が一気に増えました。\n","\n",">[参考URL]\n",">\n",">https://www.tensorflow.org"]},{"cell_type":"markdown","metadata":{"id":"TnQ9D4jD11e2","colab_type":"text"},"source":["以上で、深層学習を学ぶための準備を終わります。近年は深層学習に関する理論的な本や実装に関する良い本も出ています。参考書籍「A-30」をお勧めします。この講座を終えられた方ならスムーズに入れるのではないかと思います。"]},{"cell_type":"markdown","metadata":{"id":"kjr_gSRo11e3","colab_type":"text"},"source":["また、深層学習については参考書籍「A-30」の『深層学習』がとても有名です。なお、英語版はネットで公開されています（参考URL「B-22」）。"]},{"cell_type":"markdown","metadata":{"id":"zmDSP7Gn11e5","colab_type":"text"},"source":["## 10.3 Pythonの高速化\n","ゴール：Pythonを高速化するための方法を知る"]},{"cell_type":"markdown","metadata":{"id":"zBw4ju4a11e6","colab_type":"text"},"source":["ここからは、Pythonの処理を高速化するためのツールやアプローチについて学びます。Pythonを高速化するにはさまざまな方法がありますが、ここでは、並列処理をするmultiprocessing、コンパイラによるNumba、Cython等について説明します。"]},{"cell_type":"markdown","metadata":{"id":"aTyWdyc711e7","colab_type":"text"},"source":["Pythonの高速化については、参考文献「A-31」があります。\n","\n","1つ目の『ハイパフォーマンスPython』は、全体的なコンピューターシステム視点でPython処理のボトルネックとなる箇所を探すために、プロファイリング（システムをテストして遅い箇所を特定する）を実施したり、テクニカルな視点（リストとタプルの違いと扱い方など）でも説明がされています。\n","\n","2つ目の『科学技術計算のためのPython入門』も高速化について、いくつかの章で解説がされていますので、参考にしてください。3つ目の『エキスパートPythonプログラミング改訂2版』にも、最適化や並行処理について述べられています。"]},{"cell_type":"markdown","metadata":{"id":"rf4frneJ11e7","colab_type":"text"},"source":["ただし、Pythonによる高速化や処理の最適化は、まず動くコードを作成したあとに考えるのが原則です。システム全体として最適化をするためには、まずその全体的な動きをつかむことが重要で、局所的に最適化しても全体として最適化できるとは限りません。目的となるシステム、処理を作成した後、計算スピードに問題があるときに、検討するようにしましょう。また、やみくもに計算時間を計測するのではなく、どこに問題がありそうか仮説を立て、プロファイリングしていくことも重要です。"]},{"cell_type":"markdown","metadata":{"id":"zSjBo6CQ11e8","colab_type":"text"},"source":["### 10.3.1 並列処理\n","キーワード：並行処理、並列処理、プロセス、スレッド、GIL、multiprocessing"]},{"cell_type":"markdown","metadata":{"id":"bEt9Dxb511e9","colab_type":"text"},"source":["Python高速化のために、まずmultiprocessingのモジュールを使った処理をみてきます。\n","\n","その前に、周辺で必要となる概念（並列処理と並行処理、スレッドとプロセス、GILなど）について紹介します。ここでは、簡単な用語の説明に留めるので、詳細は先ほど紹介した参考文献等をみてください。\n","\n","#### 並行処理と並列処理\n","\n","まずは並行処理と並列処理の違いからです。並行処理と並列処理は同じような用語ですが、別の概念です。\n","\n","並行処理は複数のタスクを非同期で実行していきます。一方、並列処理は、並行処理の1つであり、複数のCPUを使って複数のタスクを同時に処理をしてきます。\n","\n","言葉の説明だけでは若干イメージがわきにくいので、以下の参照図をみてみましょう。左の図が並列処理で、右の図が並行処理です。並行処理では1つのCPUが2つのタスクを非同期で実行していますが、並列処理では、CPU1とCPU2の2つのCPUがそれぞれ同時に異なるタスクを処理しているのがわかります。"]},{"cell_type":"markdown","metadata":{"id":"tiYZyoS611e-","colab_type":"text"},"source":["![comment](https://www.dotnetcurry.com/images/dotnetcore/concurrent/parallel-vs-concurrent-dotnet-core.png)"]},{"cell_type":"markdown","metadata":{"id":"5wdbb0nn11e_","colab_type":"text"},"source":["参照URL:http://www.dotnetcurry.com/dotnet/1360/concurrent-programming-dotnet-core"]},{"cell_type":"markdown","metadata":{"id":"s1pAyIlM11fA","colab_type":"text"},"source":["複数のCPUコアを使うときに意識しなければいけないのが、**グローバル・インタプリタ・ロック（GIL）**です。これは、Pythonのプロセスはコア数に関わらず、一度に１つの命令しか実行しないことを言います。\n","\n","しかし以下で紹介するmultiprocessingの並列処理を使えば、プロセスとスレッドを使った並列処理を実現することができ、1つのマシンで複数のコアを使うことができます。なおプロセスとは、プログラムの実行単位のことで、スレッドとは、プロセスで作成される処理の単位のことをいいます。\n","\n","#### multiprocessingを使った並列処理の例\n","\n","用語の説明はここまでにして、multiprocessingのサンプルコードを実際にみていきましょう。\n","\n","**なお、環境等によって、計算時間が変わってきますので、記載通りの結果にならないこともありますが、ご了承ください。**\n"]},{"cell_type":"markdown","metadata":{"id":"dVnp9Phd11fB","colab_type":"text"},"source":["以下のコードは、入力した値を2乗してそれを表示し、0.5秒待つ処理をするものです。このプログラムは何も特別な処理をせず、普通に実装しているだけのコードです（ただし、あくまで並列処理のイメージをもってもらうための実装で、これがシステム的に最適であるとは限りません）。\n"]},{"cell_type":"code","metadata":{"id":"aanC1a_611fC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":212},"outputId":"116619de-7fe5-4b1d-95c5-7f721e290ecc","executionInfo":{"status":"ok","timestamp":1583297029890,"user_tz":-540,"elapsed":5814,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["import time\n","\n","# 入力値を二乗して結果を表示し、0.5秒待つ\n","def calc(x):\n","    a = x**2\n","    print(a)\n","    # 0.5秒待つ\n","    time.sleep(0.5)\n","\n","# ここから処理を始める\n","if __name__ == '__main__':\n","    \n","    # 計算開始時間\n","    start_time = time.time()\n","    \n","    # リストデータの作成\n","    data = [t for t in range(0, 10)]\n","\n","    # 関数の呼び出しとリスト化\n","    [calc(x) for x in data]\n","    \n","    # 計算時間の測定\n","    print('Calc Time:{0:0.3f}[s]'.format(time.time() - start_time))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["0\n","1\n","4\n","9\n","16\n","25\n","36\n","49\n","64\n","81\n","Calc Time:5.010[s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LqitD1jw11fF","colab_type":"text"},"source":["次は、multiprocessingを使って並列処理してみましょう。`multiprocessing.Process`などを使って、次のように実装します。"]},{"cell_type":"code","metadata":{"id":"Kbuxs_H-11fG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":212},"outputId":"a1b151f7-cc09-485d-b145-1ece2e179c07","executionInfo":{"status":"ok","timestamp":1583297061892,"user_tz":-540,"elapsed":2147,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["import time\n","import multiprocessing\n","\n","# 入力されたデータから以下のcalcを使ってリスト化する関数\n","def worker(data):\n","    [calc(x) for x in data]\n","\n","# 入力値を二乗して結果を表示し、0.5秒待つ\n","def calc(x):\n","    a = x ** 2\n","    print(a)\n","    time.sleep(0.5)\n","\n","# ここから処理開始\n","if __name__ == '__main__':\n","    \n","    start_time = time.time()\n","    \n","    # プロセスを分けるための設定\n","    split_data = [[0, 1, 2], [3, 4], [5, 6], [7, 8, 9]]\n","\n","    jobs = []\n","    for data in split_data:\n","        job = multiprocessing.Process(target=worker, args=(data, ))\n","        jobs.append(job)\n","        job.start()\n","\n","    [job.join() for job in jobs]\n","    \n","    print('Calc Time:{0:0.3f}[s]'.format(time.time() - start_time))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["0\n","9\n","25\n","49\n","1\n","16\n","64\n","36\n","4\n","81\n","Calc Time:1.547[s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DJXzoDD-11fJ","colab_type":"text"},"source":["計算時間は改善しました。\n","\n","#### マルチスレッドを使った例\n","\n","次に、マルチスレッドを使った処理を見ていきましょう。以下の処理では3つのURLにリクエストを送って、待ち時間の合計を計算しています。こちらは普通の実装です。"]},{"cell_type":"code","metadata":{"id":"9Q-ncsPo11fK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1bc4f5e3-0199-48cb-b8cd-67bf2ca0f095","executionInfo":{"status":"ok","timestamp":1583297080098,"user_tz":-540,"elapsed":1684,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["from urllib.request import urlopen\n","import time\n","\n","current_time = time.time()\n","urls = ['http://www.google.com', 'http://www.yahoo.co.jp/', 'https://www.bing.com/']\n","for url in urls:\n","    response = urlopen(url)\n","    html = response.read()\n","print('Calc Time:{0:0.3f}[s]'.format(time.time() - current_time))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Calc Time:0.799[s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ChpkRKFW11fP","colab_type":"text"},"source":["次にこれをマルチスレッドを使って処理してみましょう。threadingを使います。"]},{"cell_type":"code","metadata":{"id":"yZr0dxDP11fQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"83d76e37-a532-450c-efb6-80ed350b454d","executionInfo":{"status":"ok","timestamp":1583297088430,"user_tz":-540,"elapsed":1169,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["from urllib.request import urlopen\n","import threading, time\n","\n","def get_html(url):\n","    current_time = time.time()\n","    response = urlopen(url)\n","    html = response.read()\n","    print(url + ': ' + str(time.time() - current_time))\n","\n","urls = ['http://www.google.com', 'http://www.yahoo.co.jp/', 'https://www.bing.com/']\n","threads = []\n","\n","# Start Threads\n","current_time = time.time()\n","for url in urls:\n","    thread = threading.Thread(target=get_html, args=(url, ))\n","    thread.start()\n","    threads.append(thread)\n","\n","# Wait Threads\n","for thread in threads:\n","    thread.join()\n","\n","print('Calc Time:{0:0.3f}[s]'.format(time.time() - current_time))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["http://www.google.com: 0.04518294334411621\n","https://www.bing.com/: 0.25518274307250977\n","http://www.yahoo.co.jp/: 0.3511998653411865\n","Calc Time:0.353[s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o0KyBmfh11fT","colab_type":"text"},"source":["処理時間は、先ほどより改善されているのがわかります。"]},{"cell_type":"markdown","metadata":{"id":"B-ziSIl811fU","colab_type":"text"},"source":["### 10.3.2 Numba入門\n","キーワード：JITコンパイラ、numba"]},{"cell_type":"markdown","metadata":{"id":"OimmF0sY11fV","colab_type":"text"},"source":["次は、JIT（Just in Time）コンパイラを使って、高速化しましょう。ここでは、LLVMベースのコンパイラ**numba**を使います。\n","\n","高速化するには、高速化したい関数の前にデコレータ（入力として関数を受け取り別の関数を返す）の`@jit`をつけます。そうすることでJITコンパイラが機械語にコンパイルしてくれて、Cにも匹敵する計算性能が出せるようになります。\n","\n","以下に示すのは、複素数の計算をする例です。普通にPythonの機能として実装した`multi_abs_basic`とNumpyで実装した`multi_abs_numpy`、そして、先頭に「`@jit`」を付けて、`multi_abs_basic`と同じ処理をJIT化した`multi_abs_numba`の3つの関数を用意しました。"]},{"cell_type":"code","metadata":{"id":"G9T5erjG11fW","colab_type":"code","colab":{}},"source":["from numba import jit\n","import numpy as np\n","import time\n","\n","#それぞれの要素の掛け算を実行して、リストにする\n","#普通の実装\n","def mult_abs_basic(N, x, y):\n","    r = []\n","    for i in range(N):\n","        r.append(abs(x[i] * y[i]))\n","    return r\n","\n","#numpyの実装\n","def mult_abs_numpy(x, y):\n","    return np.abs(x*y)\n","\n","#JITの実装\n","@jit('f8[:](i8, c16[:], c16[:])', nopython=True)\n","def mult_abs_numba(N, x, y):\n","    r = np.zeros(N)\n","    for i in range(N):\n","        r[i] = abs(x[i] * y[i])\n","    return r"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvE4dTOc11fZ","colab_type":"text"},"source":["まずは、計算をするための変数の準備をします。以下のJは複素数を扱うために使っています。"]},{"cell_type":"code","metadata":{"id":"MTiA5uUM11fa","colab_type":"code","colab":{}},"source":["N = 1000000\n","\n","x_np = (np.random.rand(N)-0.5) + 1J*(np.random.rand(N)-0.5)\n","y_np = (np.random.rand(N)-0.5) + 1J*(np.random.rand(N)-0.5)\n","\n","x = list(x_np)\n","y = list(y_np)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-EAchWS11fd","colab_type":"text"},"source":["それではまず、普通に計算したときの処理時間を見てみましょう。"]},{"cell_type":"code","metadata":{"id":"9-UCl5U811fd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8b6ab6d6-b049-47aa-a44a-cb1cfcf24f65","executionInfo":{"status":"ok","timestamp":1583297153213,"user_tz":-540,"elapsed":1914,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["start_time = time.process_time()\n","b1 = mult_abs_basic(N, x, y)\n","print('Calc Time:{0:0.3f}[s]'.format(time.process_time()-start_time))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Calc Time:0.340[s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JIFxMGve11fi","colab_type":"text"},"source":["次は、numpyを使って計算した場合です。"]},{"cell_type":"code","metadata":{"id":"K5xDO3TN11fj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7182db8f-c69a-49c9-f00b-c004adf91335","executionInfo":{"status":"ok","timestamp":1583297159451,"user_tz":-540,"elapsed":820,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["start_time = time.process_time()\n","b1 = mult_abs_numpy(x_np, y_np)\n","print('Calc Time:{0:0.3f}[s]'.format(time.process_time()-start_time))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Calc Time:0.096[s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jNnGL09L11fl","colab_type":"text"},"source":["numpyを使った方が計算時間が大幅に改善されているのがわかります。\n","\n","最後にJITコンパイラを使って計算した時を見てみましょう。"]},{"cell_type":"code","metadata":{"id":"MZA8xAFz11fn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5000ff07-d2cf-492d-f4c6-5d367c407f95","executionInfo":{"status":"ok","timestamp":1583297167662,"user_tz":-540,"elapsed":856,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["start_time = time.process_time()\n","b1 = mult_abs_numba(N, x_np, y_np)\n","print('Calc Time:{0:0.3f}[s]'.format(time.process_time()-start_time))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Calc Time:0.034[s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ag7PD1-H11fr","colab_type":"text"},"source":["Cで実装されているNumpyよりもかなり計算時間が削減されていることがわかります。"]},{"cell_type":"markdown","metadata":{"id":"6-7s-ju111fs","colab_type":"text"},"source":["### 10.3.3 Cython入門\n","キーワード：Cython、コンパイル言語、インタープリタ言語"]},{"cell_type":"markdown","metadata":{"id":"S1Yy5ldI11ft","colab_type":"text"},"source":["Pythonの高速化の最後として、**Cython**を使ったサンプルコードを紹介します。\n","\n","PythonとCとの違いは、Pythonがインタープリタ言語に対して、Cはコンパイラ言語であるという点です。Pythonのようなインタープリタ言語はすぐに実行され、その処理結果の確認ができますが、C言語は実行前に機械語に直す過程（コンパイル）が必要であるため、その時間がかかります。\n","\n","しかし、インタープリタ言語のデメリットとしては、実行速度そのものが遅くなってしまう点が挙げられます。コンパイラ言語は、事前に機械語に置き換えるため、実行の際の速度が速くなります。\n","\n","ここで扱うCythonは、Pythonのように簡単に実装でき、コンパイルは必要になりますが、実行速度を早めることができます。\n","\n","CythonはNumpyやScipyなどのさまざまなライブラリで使われており、本来は背景やその仕組み（CPythonとの関係性等）についても詳しく記載すべきですが、本書での目的は、まずは実装して動かすということに重きをおいているために、省略します。詳細は、以下で紹介する参考文献をみてください。\n","\n","#### PythonとCythonの速度を比較する\n","\n","ここでは、PythonとCythonの計算スピードの比較をします。まずは、Jupyter Notebookにて、以下のようにマジックコマンドを実行します。この`%load_ext Cython`と以下で述べる`%%cython`を実行することで、Jupyter Notebook内でもコンパイルが可能になります。"]},{"cell_type":"code","metadata":{"id":"MyxG9hOv11fu","colab_type":"code","colab":{}},"source":["%load_ext Cython"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9H2eXu4x11fy","colab_type":"text"},"source":["ここでは以前扱った素数とフィボナッチ数を計算するプログラムをPythonとCythonで実装し、それぞれの速度の違いを比較してみましょう。\n","\n","##### Pythonによる実装\n","まずは普通のPythonによる実装です。1つ目が以前にも扱ったフィボナッチ数列の生成関数です。"]},{"cell_type":"code","metadata":{"id":"cjYRmtAn11fz","colab_type":"code","colab":{}},"source":["# フィボナッチ数列の作成\n","def pyfib(n):\n","    a, b = 0.0, 1.0\n","    for i in range(n):\n","        a, b = a + b, a\n","    return a"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPT99Jc111f1","colab_type":"text"},"source":["2つ目は素数生成の関数です。"]},{"cell_type":"code","metadata":{"id":"NyANy-IH11f2","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# 素数の作成\n","def pyprimes(kmax):\n","    p = np.zeros(1000)\n","    result = []\n","\n","    # 最大個数は1000個\n","    if kmax > 1000:\n","        kmax = 1000\n","\n","    k = 0\n","    n = 2\n","    while k < kmax:\n","        i = 0\n","        while i < k and n % p[i] != 0:\n","            i += 1\n","\n","        if i == k:\n","            p[k] = n\n","            k += 1\n","            result.append(n)\n","        n += 1\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Ajrilyr11f7","colab_type":"text"},"source":["##### Cythonによる実装\n","\n","次はCythonによる実装です。普通のPythonとの書き方の違いは、手前に`%%cython`マジックコマンドを記載し、変数の前に型（整数型、倍精度浮動小数点型など）が必要になるという点です。以下に示すように、「`cdef int`」や「`cdef double`」などのように、`cdef`の後に型を書いて、その後ろに変数名を書きます。"]},{"cell_type":"code","metadata":{"id":"VbreXDWs11f7","colab_type":"code","colab":{}},"source":["%%cython -n test_cython_code\n","# フィボナッチ数列生成（Cythonバージョン）\n","def fib(int n):\n","    cdef int i\n","    cdef double a=0.0, b=1.0\n","\n","    for i in range(n):\n","        a, b = a+b, a\n","    return a\n","\n","# 素数生成（Cythonバージョン）\n","def primes(int kmax):\n","    cdef int n, k, i\n","    cdef int p[1000]\n","    result = []\n","\n","    if kmax > 1000:\n","        kmax = 1000\n","\n","    k = 0\n","    n = 2\n","    while k < kmax:\n","        i = 0\n","        while i < k and n % p[i] != 0:\n","            i += 1\n","\n","        if i == k:\n","            p[k] = n\n","            k += 1\n","            result.append(n)\n","        n += 1\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tPHLUOLV11f-","colab_type":"text"},"source":["##### 速度の違いを確認する\n","\n","それでは、普通のPythonのコードとCythonのコードを比較してみましょう。まずは、フィボナッチ数列から計算します。"]},{"cell_type":"code","metadata":{"id":"RgAVlU_e11f-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"35f0bd05-bf45-4b5d-a6d2-a204c0731a48","executionInfo":{"status":"ok","timestamp":1583297302787,"user_tz":-540,"elapsed":4137,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["# フィボナッチ計算比較\n","# ilectでの動作は保証できませんが，google colaboratoryでは動作確認ずみです\n","\n","# 普通のPythonで実行\n","%timeit pyfib(1000)\n","\n","# Cythonで実行\n","%timeit fib(1000)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["10000 loops, best of 3: 44 µs per loop\n","100000 loops, best of 3: 2.05 µs per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zd_vXyXF11gC","colab_type":"text"},"source":["計算時間がなんと約35倍(43μsから1.25μs)も改善しています。ちなみに、1μsは100万分の1秒です。次は素数の計算です。"]},{"cell_type":"code","metadata":{"id":"fqXkTxpB11gE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"83119ded-997c-453e-ae1a-b746e3e10fdb","executionInfo":{"status":"ok","timestamp":1583297327299,"user_tz":-540,"elapsed":3306,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["# 素数計算比較\n","\n","# 普通のPythonで実行\n","%timeit pyprimes(1000)\n","\n","# Cythonで実行\n","%timeit primes(1000)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["1 loop, best of 3: 352 ms per loop\n","100 loops, best of 3: 2.32 ms per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jKw_deuP11gL","colab_type":"text"},"source":["こちらは100倍以上(272 msから2.19 ms)の改善となりました。ちなみに、1msは1000分の1秒です\n","\n","Cythonについては参考文献「A-32」が参考になります。"]},{"cell_type":"markdown","metadata":{"id":"mx_M-ODc11gM","colab_type":"text"},"source":["以上で、Pythonの高速化は終わりです。他にも、Pythonで高速化するためのツールとして、分散処理をするDaskやBlazeなどがありますので、興味のある方は参考文献「A-33」や参考URL「B-23」などを調べてみてください。"]},{"cell_type":"markdown","metadata":{"id":"I5p0B5Qy11gN","colab_type":"text"},"source":["冒頭でも述べましたが、ここで紹介した実装が必ず最適というわけではありません。Pythonでの計算に時間がかかる時は、上記のような手法を用いて、高速化できるかどうか検討して試してみてください。"]},{"cell_type":"markdown","metadata":{"id":"b_DEslla11gP","colab_type":"text"},"source":["## 10.4 Spark入門\n","ゴール：Sparkの機能について知る、PySparkの基礎的な機能を使える"]},{"cell_type":"markdown","metadata":{"id":"BCSLMVhG11gR","colab_type":"text"},"source":["Sparkは分散処理をするためのツールでビッグデータを解析するためのソフトウェアの1つです。機械学習のライブラリも扱え、しかもSQLのように簡単に集計ができたり、リアルタイムに分析できます。\n","\n","SparkはScalaベースで作られていますが、Scala以外にもPythonやJavaなどからも利用可能です。本講座では、Pythonとの連携をメインとして、その処理方法の基礎を学びます。特にPysparkについて、その簡単な使い方を紹介します。\n","\n","今までは、NumpyやScipy、Pandas、Scikit-learnなどを使って、データの加工処理から機械学習のモデリングを実施してきました。扱ってきたデータもそれほど大きくなく、せいぜい数ギガ程度の大きさでした。しかし、データ分析の現場では、数百ギガやテラバイト級のデータがあり、それを扱う場面も多々あり、先ほどのライブラリだけでは、データが読み込めない、計算が終わらない、困難というケースもあります。\n","\n","そこで、そのような大量のデータの読み込み、加工、機械学習まで一貫して実行するためのSparkを使うことで、ビッグデータを扱うことが可能となります。"]},{"cell_type":"markdown","metadata":{"id":"T4juYO3v11gS","colab_type":"text"},"source":["### 10.4.1 PySpark入門\n","キーワード：RDD、key/value、mapreduce、sparkSQL"]},{"cell_type":"markdown","metadata":{"id":"7xVjIIY611gT","colab_type":"text"},"source":["ここでは、PythonとSparkを連携させたPySparkの使い方について、基礎の基礎を学びましょう。PySparkはPythonからSparkを操作できるAPIです。\n","\n","#### Pysparkを使うための準備\n","\n","まずは、PySparkを使うために、以下のコードを実行しましょう。PySparkを実行するための環境構築は各自でお願いします。"]},{"cell_type":"code","metadata":{"id":"NEhuJl6Z4Ims","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"af71ce9b-a9a7-4f6d-a4bc-d8fb8ae1fb3b","executionInfo":{"status":"ok","timestamp":1583297377745,"user_tz":-540,"elapsed":5552,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["pip install findspark"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Collecting findspark\n","  Downloading https://files.pythonhosted.org/packages/b1/c8/e6e1f6a303ae5122dc28d131b5a67c5eb87cbf8f7ac5b9f87764ea1b1e1e/findspark-1.3.0-py2.py3-none-any.whl\n","Installing collected packages: findspark\n","Successfully installed findspark-1.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0ggh368J404V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":247},"outputId":"7c08e026-116c-4a71-dc54-5ae26263837c","executionInfo":{"status":"ok","timestamp":1583297601286,"user_tz":-540,"elapsed":43473,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["pip install pyspark"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n","\u001b[K     |████████████████████████████████| 217.8MB 60kB/s \n","\u001b[?25hCollecting py4j==0.10.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n","\u001b[K     |████████████████████████████████| 204kB 47.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=bfae8a60bb9d447617ebc7913ec2ffd0c7b910bb72eedd9ed22c23e146ae6503\n","  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.7 pyspark-2.4.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6u5HMyiJ11gT","colab_type":"code","colab":{}},"source":["# ilectでは動作確認できていません．各自で必要に応じて環境構築をお願いします．\n","import findspark\n","#findspark.init()\n","\n","import pyspark\n","sc = pyspark.SparkContext(appName='myAppName')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIvyi1eE11gb","colab_type":"text"},"source":["以下のようにscコマンドを入力するとパージョン番号が表示されるので、spark（pyspark）が使えるようになったことを確認できます。"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"DkLjS1hb11gc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"958184ff-8b5a-4faa-b4c1-a23f2a9084a0","executionInfo":{"status":"ok","timestamp":1583297615722,"user_tz":-540,"elapsed":854,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["sc"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://6bc8ae39a857:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v2.4.5</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>myAppName</code></dd>\n","            </dl>\n","        </div>\n","        "],"text/plain":["<SparkContext master=local[*] appName=myAppName>"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"u6vQts_o11gg","colab_type":"text"},"source":["#### データの読み込み\n","\n","ここから、分析対象のデータの読み込みをします。`sc.textFile`の後にデータのある場所を指定しています。なお、ファイル名にアスタリスク(✴︎)をつけて任意の文字を含むファイル名をまとめて読みこむことが可能です。同じような規則性のあるファイル名を扱う時は便利です。\n","\n","以前扱った「student-mat.csv」と「student-por.csv」を同時に読み込んでみましょう。ただし、ここでは重複等は気にせず、そのまま読み込みます。**なお、該当のデータがある場所は絶対パスで指定してください。**"]},{"cell_type":"code","metadata":{"id":"QtHr-CFy11gh","colab_type":"code","colab":{}},"source":["merge_student_data = sc.textFile('/root/userspace/data/student*.csv') "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1hh5QOQ11gj","colab_type":"text"},"source":["上記ではRDD（Resilient Distributed Dataset）としてデータをロードさせています。日本語では、「不変・並列実行可能な（分割された）コレクション」という意味です。RDDをベースにSparkはデータを分散処理させます。Sparkでは、このようなRDDの生成、既存のRDDの変換、結果を集計などをするためにRDDに対する呼び出しをしています。この段階ではまだ集計等はしていません。\n","\n","次は、行数を数えています。ここで集計を開始します。"]},{"cell_type":"code","metadata":{"id":"fSr7wE_011gk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":996},"outputId":"a1b125c7-7093-4246-8331-456658b7ba83","executionInfo":{"status":"error","timestamp":1583297629591,"user_tz":-540,"elapsed":1485,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["# Line count\n","merge_student_data.count()"],"execution_count":24,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-676e81658e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerge_student_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input Pattern file:/root/userspace/data/student*.csv matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"]}]},{"cell_type":"markdown","metadata":{"id":"XJkrsyuq11gn","colab_type":"text"},"source":["以下でデータの始めの行を表示します。RDDの後に`first()`をつけて実行します。"]},{"cell_type":"code","metadata":{"id":"3daa4sae11go","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":801},"outputId":"3caef377-e525-415e-b2a8-56fd01f769a5","executionInfo":{"status":"error","timestamp":1583297630871,"user_tz":-540,"elapsed":1843,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["merge_student_data.first()"],"execution_count":25,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-632462e285e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerge_student_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \"\"\"\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[1;32m   1326\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input Pattern file:/root/userspace/data/student*.csv matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"]}]},{"cell_type":"markdown","metadata":{"id":"9GVTikPl11gq","colab_type":"text"},"source":["`take(5)`で5行を抽出します。"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"BLytUxkX11gq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":801},"outputId":"540e56d0-0e19-4f8c-bd2f-7032b999fa5c","executionInfo":{"status":"error","timestamp":1583297634462,"user_tz":-540,"elapsed":844,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["merge_student_data.take(5)"],"execution_count":26,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-d2ca142e9e63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerge_student_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[1;32m   1326\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input Pattern file:/root/userspace/data/student*.csv matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"]}]},{"cell_type":"markdown","metadata":{"id":"t2BZtn7f11gu","colab_type":"text"},"source":["次は、`lambda`も組み合わせて、フィルターをかけます。具体的には、GPという文字が含まれている行を抽出します。"]},{"cell_type":"code","metadata":{"id":"6tbvTyLx11gu","colab_type":"code","colab":{}},"source":["gp_lines = merge_student_data.filter(lambda line: 'GP' in line)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pywRUVG811gw","colab_type":"text"},"source":["補足ですが、まだ上のフィルターの段階では、計算はしておらず、上記の`count`や`first`を実行した時に計算がされます。これはSparkの、「実際に値が必要になるまでは計算をしない」という**遅延評価**によるものです。先ほどの、`take()`も実施した時に、計算がされます。Sparkではこの考え方がとても重要なので、押さえておいてください。\n","\n","以下は行数をカウントしています。ここで、計算が実行されます。"]},{"cell_type":"code","metadata":{"id":"KxTG2tK411gx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":996},"outputId":"e64b8af5-ea83-4b6b-8285-77deacf63086","executionInfo":{"status":"error","timestamp":1583297640979,"user_tz":-540,"elapsed":1146,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["gp_lines.count()"],"execution_count":28,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-1edb30ed55d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgp_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input Pattern file:/root/userspace/data/student*.csv matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"]}]},{"cell_type":"markdown","metadata":{"id":"zz2CdSM-11g1","colab_type":"text"},"source":["上記の結果より、GPが含まれる行数は772であることがわかりました。\n"]},{"cell_type":"markdown","metadata":{"id":"gsO5Pvxr11g2","colab_type":"text"},"source":["また、カラム名が入っている行をカウントしましょう。先ほど2ファイルをそのまま読み込んでいるので、カラム名が入っている行は2行あるはずです。以下は、カラム名の1つである`school`が含まれている行数をカウントしています。\n"]},{"cell_type":"code","metadata":{"id":"pWluws5W11g2","colab_type":"code","colab":{}},"source":["head_lines = merge_student_data.filter(lambda line: 'school' in line)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U34e6PKu11g4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":996},"outputId":"9cba242b-3ac5-4e2b-cd73-d2605afeaed5","executionInfo":{"status":"error","timestamp":1583297644295,"user_tz":-540,"elapsed":624,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["head_lines.count()"],"execution_count":30,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-7635a9df900b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhead_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input Pattern file:/root/userspace/data/student*.csv matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6HPlgOrv11g-","colab_type":"text"},"source":["これを応用して、MapReduceの処理を見てみましょう。以下では、それぞれのセルに入っているデータ（数字も）を1つの単語と見なし、それぞれいくつあるのかをカウントする処理をしています。\n","\n","まず、splitを使って「;」で文字を分けています。"]},{"cell_type":"code","metadata":{"id":"FQfq03Hu11g_","colab_type":"code","colab":{}},"source":["split_words = merge_student_data.flatMap(lambda line:line.split(';'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWus-c3h11hB","colab_type":"text"},"source":["次に、`map`で単語1つにカウント1を対応させ、`reduce`の処理でそれぞれ同じ単語のカウントを足しあげています。これがMapReduceの処理です。キーとなっているのが単語（`word`）です。"]},{"cell_type":"code","metadata":{"id":"xPGulTEU11hC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":801},"outputId":"d796ce82-4dd0-4334-d390-7a366aa93fd9","executionInfo":{"status":"error","timestamp":1583297648835,"user_tz":-540,"elapsed":659,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["word_counts = split_words.map(lambda word:(word, 1)).reduceByKey(lambda a, b: a+b)"],"execution_count":32,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-9ab04879575a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \"\"\"\n\u001b[0;32m-> 1625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduceByKeyLocally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcombineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \"\"\"\n\u001b[1;32m   1852\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m             \u001b[0mnumPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defaultReducePartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2263\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input Pattern file:/root/userspace/data/student*.csv matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Fdzjeazo11hF","colab_type":"text"},"source":["`collect()`でRDD全体を取り出しますので、大きなデータを扱う際は注意してください。"]},{"cell_type":"code","metadata":{"id":"_HsesuOV11hF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":172},"outputId":"6b7c8d33-ad8a-455d-f4c2-f6d73abb1097","executionInfo":{"status":"error","timestamp":1583297649899,"user_tz":-540,"elapsed":634,"user":{"displayName":"Ryo3 T","photoUrl":"","userId":"09896688176400106919"}}},"source":["result = word_counts.collect()"],"execution_count":33,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-b7c52ca2fc46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'word_counts' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"8lsrcUux11hI","colab_type":"text"},"source":["返ってくる値はリスト型です。数が多いので表示結果は絞っています。"]},{"cell_type":"code","metadata":{"id":"zmB_re0Y11hI","colab_type":"code","colab":{}},"source":["result[0:10]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YkNsOxQp11hK","colab_type":"text"},"source":["また、Statisticsライブラリ等を使うことで、基本統計量の計算も可能です。"]},{"cell_type":"code","metadata":{"id":"d7mVdJIS11hL","colab_type":"code","colab":{}},"source":["from pyspark import SparkContext\n","from pyspark.mllib.linalg import Vectors\n","from pyspark.mllib.stat import Statistics\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v0RobIhg11hO","colab_type":"text"},"source":["まずは、統計量を計算するためのデータを準備します。"]},{"cell_type":"code","metadata":{"id":"HvI4hGFh11hP","colab_type":"code","colab":{}},"source":["rdd = sc.parallelize([Vectors.dense([2, 0, 0, -2]), Vectors.dense([4, 5, 0, 3]), Vectors.dense([6, 7, 0, 8])])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FETV4CTq11hR","colab_type":"text"},"source":["次に、 `Statistics.colStats`を使って、基本統計量等を計算します。"]},{"cell_type":"code","metadata":{"id":"1Mr02aFj11hR","colab_type":"code","colab":{}},"source":["summary = Statistics.colStats(rdd)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6-aBmGLm11hV","colab_type":"text"},"source":["以下は、順番にベクトルの各要素の平均、分散、ゼロでない数のカウントを計算しています。"]},{"cell_type":"code","metadata":{"id":"IEPEJn4p11hV","colab_type":"code","colab":{}},"source":["print (summary.mean())\n","print (summary.variance())\n","print (summary.numNonzeros())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v07v6KUO11hZ","colab_type":"text"},"source":["以上で、データの読み込みと簡単な集計の紹介は終わります。次は、SparkSQLを見ていきます。"]},{"cell_type":"markdown","metadata":{"id":"kqhVuH_M11hZ","colab_type":"text"},"source":["#### <練習問題 10-2>\n","\n","10.4.1 「PySpark入門」で使用したデータ「merge_student_data」に対して、`schoolsup`を含むレコードがどれだけあるかカウントしてください。"]},{"cell_type":"markdown","metadata":{"id":"FeNIlld111ha","colab_type":"text"},"source":["### 10.4.2 SparkSQL入門\n","キーワード：SparkSQL"]},{"cell_type":"markdown","metadata":{"id":"Rh-8SZvh11hb","colab_type":"text"},"source":["Sparkは、データベース（DB)のデータを操作するSQLも扱うことができます。以下は、そのモジュールの読み込みと準備をしています。なお、以下は、DBの基礎やSQLの基礎がある方を前提に説明をしていますので、あらかじめご了承ください。\n"]},{"cell_type":"code","metadata":{"id":"UY70rruV11hd","colab_type":"code","colab":{}},"source":["# ilectでは動作確認できていません．各自で必要に応じて環境構築をお願いします．\n","from pyspark.sql import SQLContext\n","from pyspark.sql.types import StructField, StringType, IntegerType, StructType, FloatType\n","sqlContext = SQLContext(sc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_vtWpNM11hf","colab_type":"text"},"source":["3章で扱ったデータを対象に、SQL（ここではSparkSQL）を使って集計してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"yoMdpmiP11hg","colab_type":"text"},"source":["データを読み込みます。RDDとしてロードしています。なお、該当のデータがある場所は絶対パスで指定してください。"]},{"cell_type":"code","metadata":{"id":"e3HgZlnC11hh","colab_type":"code","colab":{}},"source":["student_mat_data = sc.textFile('/root/userspace/data/student-mat.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8TIh83JL11hk","colab_type":"text"},"source":["見出し列だけ読み込んで確認してみましょう。"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"982nFk8X11hk","colab_type":"code","colab":{}},"source":["header = student_mat_data.first()\n","header"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYfe4JFL11hm","colab_type":"text"},"source":["このデータの区切り文字は特殊でした。以下のように;を置き換えましょう。`replace`を使います。"]},{"cell_type":"code","metadata":{"id":"EjuPaPKa11hn","colab_type":"code","colab":{}},"source":["schemaString = header.replace(';', ',') "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CV6LvO-d11ho","colab_type":"code","colab":{}},"source":["schemaString"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rb0omaxK11hq","colab_type":"text"},"source":["さて、データベースのテーブルを準備するとき、フィールド名や型をすべて指定しなければいけませんが、30個以上もあるので、設定するのは大変です。少し乱暴ですが、以下のようにプログラムを書いて、作業を効率化しましょう。取り急ぎすべて文字型にしています。個別に後から変更することも可能です。"]},{"cell_type":"code","metadata":{"id":"MzXIV4KA11hq","colab_type":"code","colab":{}},"source":["fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split(',')]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CxPU4w2v11hs","colab_type":"text"},"source":["これをデータ構造としてスキーマを定義します。"]},{"cell_type":"code","metadata":{"id":"AeP5OJF311ht","colab_type":"code","colab":{}},"source":["schema = StructType(fields)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggCJc8iE11hv","colab_type":"text"},"source":["先ほど確認した見出し列を除いたデータを準備します。"]},{"cell_type":"code","metadata":{"id":"QKA1wpPK11hw","colab_type":"code","colab":{}},"source":["Header = student_mat_data.filter(lambda l: \"school\" in l)\n","NoHeader = student_mat_data.subtract(Header)\n","NoHeader.count()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gfROHzdT11hz","colab_type":"text"},"source":["以下は、見出し列を除いたデータの1行目です。"]},{"cell_type":"code","metadata":{"id":"IYeGuipI11h1","colab_type":"code","colab":{}},"source":["NoHeader.first()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TcF884f_11h3","colab_type":"text"},"source":["また次に、文字が”で囲まれているため、`map`関数と`lambda`関数を使って、その文字を消去しています。"]},{"cell_type":"code","metadata":{"id":"o9nR0doL11h3","colab_type":"code","colab":{}},"source":["NoHeader2 = NoHeader.map(lambda l: l.replace('\\\"', ''))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0WmLTpT11h5","colab_type":"code","colab":{}},"source":["NoHeader2.first()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BciLKxdO11h7","colab_type":"text"},"source":["さらに、;で文字が区切られているため、;で分割します。"]},{"cell_type":"code","metadata":{"id":"4CefZlW311h7","colab_type":"code","colab":{}},"source":["NoHeader3 = NoHeader2.map(lambda l: l.split(';'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dBG-Y-Yk11h9","colab_type":"text"},"source":["次は、上のデータをデータフレームにします。先ほどのスキーマで設定します。`sqlContext.createDataFrame`を使います。"]},{"cell_type":"code","metadata":{"id":"Quz0vzsJ11h-","colab_type":"code","colab":{}},"source":["data_frame = sqlContext.createDataFrame(NoHeader3, schema)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5lTjtqhR11iA","colab_type":"text"},"source":["最後に、SQLを使うために、上のデータフレームをテーブルにして、登録します。"]},{"cell_type":"code","metadata":{"id":"O2PXUDXh11iB","colab_type":"code","colab":{}},"source":["data_frame.registerTempTable('tmp_table')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ivp_tfAb11iD","colab_type":"text"},"source":["以下で、スキーマを確認します。"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"4mKiFI4311iD","colab_type":"code","colab":{}},"source":["print(data_frame.printSchema())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3tCYLciC11iF","colab_type":"text"},"source":["それでは、SQLの`select`文を使ってみましょう。基本的に`sqlContext.sql`の中でSQLを記述すれば、大丈夫です。なお、集計結果としては、前半のChapterで実行した結果と同じになります。"]},{"cell_type":"code","metadata":{"id":"A_NuQTvM11iF","colab_type":"code","colab":{}},"source":["count_result = sqlContext.sql('select sex,avg(age) as avgAge from tmp_table group by sex')\n","print(count_result.show())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"POONRUpj11iI","colab_type":"text"},"source":["以上で、Sparkの説明は終わりになります。Sparkではさらに、前に学んだ機械学習の手法もMLibを使ってRDDに適応させることが可能です。\n","\n","今回は簡単な実装の紹介のみで、一般的な環境ではSparkの凄さを実感できませんが、将来的にAmazon Web Services（AWS、Amazonが提供するクラウドサービス）などでたくさんのサーバーが使えるときや分散処理を実施する時に、ぜひ使ってみてください。ちなみに、Sparkはインストール等が少し大変だと思うので、AWSが提供するAmazon Elastic MapReduce（EMR）が便利です。ノード数の指定などが簡単に設定できるので、はじめて使う方にはEMRはオススメです。\n","\n","またSparkはScalaベースで作られており、色々な面でScalaからの方が扱いやすいと思いますので、本格的に使われる場合はScalaもプログラミング言語の選択肢として考えるのも良いと思います（PySparkは制約があります）。\n","\n","なお、参考文献「A-34」もご覧ください。"]},{"cell_type":"markdown","metadata":{"id":"EfcUL-dG11iJ","colab_type":"text"},"source":["#### <練習問題 10-3>\n","\n","SparkSQLを使って、先ほど作ったテーブルに対して、`school`× `sex`を軸にして、それぞれのレコード数と、それぞれの平均年齢を表示するようにしてください。"]},{"cell_type":"markdown","metadata":{"id":"cRQD02Js11iK","colab_type":"text"},"source":["## 10.5 その他の数学的手法とエンジニアリングツール\n","ゴール：データ分析のアプローチ方法（数学的手法とエンジニアリング）を広げる"]},{"cell_type":"markdown","metadata":{"id":"4QuuavCX11iL","colab_type":"text"},"source":["### 10.5.1 数学的手法\n","キーワード：MCMC、階層ベイズ、実験計画法、生存解析、確率過程とランダムウォーク、時系列解析、トピックモデル"]},{"cell_type":"markdown","metadata":{"id":"vKXA7vWO11iL","colab_type":"text"},"source":["この講座では、データ分析、特に機械学習の分野について、最低限必要な手法を学んできました。さらに、いろいろな課題に対するアプローチを増やすために、数学的な手法とそれらを実装するライブラリ等について紹介します。本書の冒頭で紹介したデータサイエンティストに必要なうちの1つのデータサイエンス（数学・統計モデリング）をスキルアップさせるためのコンテンツ紹介になります。ここでは、ほんの数行程度の説明ですが、本書読了後に、業務上必要そうなもの、興味があるものをぜひ学んでいってください。"]},{"cell_type":"markdown","metadata":{"id":"T4Abgphn11iM","colab_type":"text"},"source":["- **マルコフ連鎖モンテカルロ法（MCMC）**：この手法は乱数を使ってマルコフ連鎖（ある状態が直前の状態にのみ依存して、その連鎖を確率モデルで表したもの）を発生させる方法で、多重積分を計算するときなどに使われます。PythonのライブラリではPyMC3から使うことができます。なお、教師なしで紹介したクラスタリングですが、EMアルゴリズムといわれる方法と関係があり、またこのMCMCの一種であるギブス標本抽出も関係していますので、参考文献「A-35」等を見てください。\n"]},{"cell_type":"markdown","metadata":{"id":"Mv5cXg8N11iM","colab_type":"text"},"source":["- **階層ベイズ**：統計モデルにはパラメータ（母平均など）がありましたが、それに階層構造をもたせてベイズ推定する手法が階層ベイズです。単純にパラメータを固定するだけではうまく現象を説明できないこともあるため、その背後にある確率分布をさらに考えます。ライブラリは先ほどのPyMC3などを使えば、階層ベイズを使うことができます。"]},{"cell_type":"markdown","metadata":{"id":"vvz-vwz911iO","colab_type":"text"},"source":["- **実験計画法**：ある現象が生じた場合に、その原因は何なのか、それらには本当に因果関係があるのかどうか、実際の実験を通して観察し、その結果が妥当であるかどうかを分析するのが実験計画法です。実験計画法では少ない実験回数で効果的な情報を得られるように実験計画を工夫します。\n","\n","    相関があるからといって因果関係があるとはいえません。この実験計画法によって、ある現象がある要因によって生じているのかどうかを調べていきます。マーケティング分野ではコンジョイント分析という名で利用されています。具体的な例としては、消費者がパソコンを購入する際、価格や色、デザイン、品質などが購入するかしないかに影響してきますが、それらの影響の度合いを調べるための効率的な実験を、直交表という表を用いて計画します。\n","\n","    他には、スーパーなどで発行されるクーポンの効果を見るために、ある店舗群はクーポンを配り（処置群）、別の同じような売り上げの店舗群はクーポンを発行しない（コントロール群）という実験等が行われています。"]},{"cell_type":"markdown","metadata":{"id":"KweJopmL11iO","colab_type":"text"},"source":["- **時系列解析**：時間とともにランダムに変動するデータを分析するのが、時系列解析です。分析対象の主なデータは、気候（気温、雨量など）、株価や地震波、売上推移などさまざまな時系列データです。それらの移動平均を計算したり、それ自身の過去のデータと相関をとって分析（自己相関）したり、多変量の時系列を解析して、将来の予測計算をします。これを行えるPythonのライブラリとしては、StatsModelsがあります。"]},{"cell_type":"markdown","metadata":{"id":"6pHUusnd11iP","colab_type":"text"},"source":["- **確率過程とランダムウォーク、確率解析**：ファイナンス理論（オプション価格のデリバティブ計算等）に応用されることが多く、金融業界でクオンツといわれる職種を目指される方は、これらの領域を学ぶことになります。確率微分方程式の離散化バージョンなどは以下の参考文献などに記載されています。以下の参考URL「B-24」のGitHubに載せているコンテンツも参考になりますし、また、参考URL「B-25」の**Quantopian**もコンテンツがとても充実していますので、金融で必要な実装を学ぶことができ、オススメです。"]},{"cell_type":"markdown","metadata":{"id":"dv4IVIaM11iQ","colab_type":"text"},"source":["- **生存時間解析**：その名の通り、ある分析の対象がどれだけ生き続けているか（人や製品なども含む）を解析するためのアプローチです。生存曲線という、時間と生存する確率を対応させた関数を使いますが、生存時間の分布があらかじめ分かっていることは少ないため、それを推定するための手法として、カプランマイヤー推定などが使われます。この手法は、あるイベントが発生するまでの時間を解析するための方法で、医療の分野では生存率を評価するときなどに使われているようです。これを行えるPythonのライブラリとしてはLifelinesがあります。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LsPDh16p11iR","colab_type":"text"},"source":["- **トピックモデル**：自然言語処理の一分野でもあり、ニュースなどたくさんの記事があった時に、それらが一体何のトピックなのか分析するのがトピックモデルです。応用分野としては、ニュース記事の分類以外にも、購買行動の分析（セグメンテーションなど）もあります。これを扱うPythonのライブラリとしては、gensimがあります。なお自然言語処理は、NLTKライブラリがよく使われます。"]},{"cell_type":"markdown","metadata":{"id":"UhmnZv9i11iR","colab_type":"text"},"source":["以上、簡単ではありましたが、その他の数学的な手法の紹介は終わります。他にも、さらなる応用領域（ベイジアンネットワーク、状態空間モデルとカルマンフィルタ、マリアバン解析等）もあります。興味のある方は、参考文献「A-35」も参考にしてください。\n","\n","また、特にオススメなのが、先ほども紹介した参照URL「B-25」のQuantopianのWebサイトです。ファイナンス系のサイトですが、Jupyter Notebookを使って手を動かして学べる環境があります。しかも無料です。ファイナンス専門家の方はもちろん、専門外の方もいろいろと学ぶことが多いと思いますので、オススメです。"]},{"cell_type":"markdown","metadata":{"id":"okSL2-yt11iS","colab_type":"text"},"source":["### 10.5.2 エンジニアリングツール\n","キーワード：Linux、AWS、Hadoop、FPGA、Scala、R、Java、OpenCV、Tableau、PowerBI"]},{"cell_type":"markdown","metadata":{"id":"10IIkI5X11iS","colab_type":"text"},"source":["今までは主にPythonを使ってきましたが、分析に関連するソフトウェアや分析に役立つツールを、まばらではありますが紹介します。ここの講座のみですべてを学ぶことはできませんが、将来的なアンテナを張れるように単語を知っておくだけでもためになると思います。ここは、データサイエンティストになるために必要なスキルの2つ目エンジニアリング力を磨くためのコンテンツ紹介になります。\n"]},{"cell_type":"markdown","metadata":{"id":"IOyztDHS11iT","colab_type":"text"},"source":["- **Linux（コマンドライン、シェル）**：OSの1つです。本書でもAppendixなどで少しコマンドラインを使いました。将来、分析をするだけではなく、分析をするための環境構築等にも必要になってくるスキルです。初学者にはなかなかハードルが高いですが、Linuxを徐々に使い慣れていけば、分析環境を構築したり、実際に分析するのに、色々と選択肢が広がるでしょう。"]},{"cell_type":"markdown","metadata":{"id":"5D4D0kXD11iT","colab_type":"text"},"source":["- **Hadoop**：分散処理システムです。よく取り上げられる簡単な例が、ある文章の中に出てくる単語をそれぞれカウントするときにどうやって効率よく数えていくのかという問題です。\n","\n","    処理としては、文章の中にある単語に数字を対応させ（Map処理）、後から集めて集計する（Reduce処理）という処理をHadoopがやります（分散処理と監視など）。単語のカウントは、上記のSparkの例でも扱いました。また、Sparkは、このHadoopと合わせて使われることも多いです。\n","\n","    ビックデータ分析といえばHadoopというくらい、大量のデータを処理するのによく紹介されます。ただ、Hadoopはバッチ処理になりますので、リアルタイムに分析をする場合には、Spark等と組み合わせる必要があります。"]},{"cell_type":"markdown","metadata":{"id":"SInrtpOx11iU","colab_type":"text"},"source":["- **FPGA**：昨今、深層学習計算などでたくさんのCPUやGPUが使われており、ソフトウェアの面からだけではなく、ハード面で実装をすることが求められています。\n","\n","    このFPGAはプログラム可能なICで、Field Programmable Gate Arrayの略です。論理仕様をプログラムすることができ、ユーザーの思い通りの論理回路を実現できる論理デバイスです。メリットとしては、CPUが処理する段階でロジックを組むことができるので、処理スピードがかなり改善されます。\n","\n","    ハードウェア記述言語のVHDLなどで書かれており、始めるのにかなりハードルが高いツールですが、最近はPynqなどPythonからも実装ができるようになってきているようです。FPGAは、データ分析中級者向けというより、レベル的にはかなりの上級者（トップクラスのデータサイエンティストかエンジニア）向けだと思いますが、本書を読了した人の中からデータ分析の第一線で活躍している人が出てくることを期待して、紹介しました。\n","\n","    インフラよりの話ではありますが、分析をするのにCPUレベルで考えなければいけない時代が来るのかもしれません。応用分野としては、MicrosoftのBingの検索エンジンやゲノム科学解析、ゲームユーザーの振り分け、金融の高頻度取引などに使われており、いろいろな企業が注目しています。ただこれも用途やデータに応じて使うものですので、FPGA（やGPUなど）を使えば必ず速くなるというものではないので注意しましょう。"]},{"cell_type":"markdown","metadata":{"id":"VVpC3CAO11iU","colab_type":"text"},"source":["- **AWS**：Amazonのクラウドサービスです。サーバー構築やデータストレージ環境などさまざまなサービスが提供されています。あるイベントが起きたときにメールを通知することができるなど、機能も多種多様です。クラウドサービスを提供する会社の中では今のところシェアが圧倒的に高いです。試験的に何か始める時には、コストを抑えることができるため便利です。また、一時的に大量のサーバーを立ち上げるなど、分散処理システムを構築したい場合に使いやすいです。\n","\n","    HadoopやSparkがあらかじめインストールされるAmazon EMRや、データベースを分散処理により高速化させるRedshiftなど、分析で使われることも多いです。また、Machine Learning（機械学習）の機能などもありますが、現状は一部的な機能だけで、ここで学んだ人には物足りないかもしれません。なお、先ほどのFPGAは、2016年12月からAWSでも使えるようになっていますが色々と制約はあるので、注意しましょう。"]},{"cell_type":"markdown","metadata":{"id":"TqTXaT2l11iV","colab_type":"text"},"source":["- **R**：統計ソフトウェアです。統計的な手法のライブラリが豊富です。変数選択法など、Pythonのパッケージではできないものもいくつかあります。もしこれらのパッケージをPythonで利用したい場合は、PythonからRのスクリプトも呼び出すことができます。"]},{"cell_type":"markdown","metadata":{"id":"tHtFxZTV11iW","colab_type":"text"},"source":["- **可視化ツール**：Tableau、PowerBIなどがあります。近年はインフォグラフィックスというデータの可視化が注目されており、これらのツールを使うことで、簡単にデータを可視化することができます。"]},{"cell_type":"markdown","metadata":{"id":"LtjnKF4t11iX","colab_type":"text"},"source":["- **OpenCV**：画像を処理するためのライブラリです。人の顔を認識したりすることもできます。Pythonと連携が可能で、Jupyter Notebookと組み合わせて実行すると便利です。"]},{"cell_type":"markdown","metadata":{"id":"ydqhQid611iX","colab_type":"text"},"source":["- その他：ここで紹介したツールやプログラミング言語の他にも、Java、Rudy、PHP、Scalaや機械学習や統計分析向けのJulia、Jumpも分析用のツールとしてよく使用されているようです。クラウドサービスとしてはAWSの他に、GoogleのGCPやIBMのBluemix、MicrosoftのAzureなどがあります。\n","\n","商用の統計解析ツールとしては、SAS、SPSS、Matlabなどがありますが、高額なので一般で買うのは難しいかもしれません。ただ、このような統計ソフトウェアの機能もPythonのライブラリで提供されているものが多くなってきましたので、PythonやRでもほとんど実現可能です。Pythonにはない統計手法を使いたい場合や、これらのソフトウェアに興味のある方、大学や会社で使う必要があるという方は、無償版がありますので、それらをインストールしたり、本などをみて勉強してください。参考文献を「A-36」にまとめました。\n","\n","他、Pythonのライブラリ関連で、Excelを扱うための`openpyxl`や、昨今流行りのRPAツールを作成するための`pyautogui`などもあります。これらを使うことで、社内にあるExcelのファイル処理やそのための自動化ツールを作成できるでしょう。興味のある方はこれらのキーワードで調べてみてください。"]},{"cell_type":"markdown","metadata":{"id":"2wkqVm1P11iX","colab_type":"text"},"source":["以上で、この章は終わります。お疲れ様でした。"]},{"cell_type":"markdown","metadata":{"id":"OtE76W9T11iY","colab_type":"text"},"source":["## 10.6 総合問題"]},{"cell_type":"markdown","metadata":{"id":"n98C0s5C11ia","colab_type":"text"},"source":["### ■総合問題10-1 深層学習の用語\n","\n","深層学習に関する以下の用語について、それぞれの役割やその意味について述べてください。また、ネットや参考文献等も使って調べてみてください。\n","- パーセプトロン\n","- ニューラルネットワーク\n","- 勾配法\n","- バッチ学習\n","- ミニバッチ学習\n","- 確率的勾配降下法\n","- 誤差逆伝播法\n","- Chainer、Theano、TensorFlow"]},{"cell_type":"markdown","metadata":{"id":"zaVJz6tD11ia","colab_type":"text"},"source":["### ■総合問題10-2 Pythonの高速化とSparkに関する用語\n","\n","Pythonの高速化とSparkに関する用語について、それぞれの役割や意味について述べてください。また、ネットや参考文献等も使って調べてみてください。\n","- 並列処理\n","- JITコンパイル\n","- Cython\n","- Spark\n","- RDD\n","- PySpark\n","- SparkSQL"]}]}